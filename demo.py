# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/187kyi5Dv7B--qM7AUT3rk5SMDiFZhMpD
"""

from transformers import pipeline
import torch

model_id = "openai/whisper-large-v3"  # update with your model id
device = "cuda:0" if torch.cuda.is_available() else "cpu"

pipe = pipeline("automatic-speech-recognition", model=model_id, device=device)

def transcribe_live_audio(audio_data, state=""):
    # Initialize the OpenAI client

    # Open the audio file in binary read mode
    output = pipe(
        audio_data,
        max_new_tokens=256,
        generate_kwargs={"task": "translate"}
    )
    text = output["text"]
    state += text + " "
    return state, state

import gradio as gr
import base64
import time
def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

with gr.Blocks() as demo:

    with gr.Row():
        with gr.Column():
            # Live audio recording and transcription
            gr.Interface(
                fn=transcribe_live_audio,
                inputs=[
                    gr.Audio(sources=["microphone"], type="filepath", streaming=True),
                    'state'
                ],
                outputs=[
                    gr.Textbox(label="Real-time Transcription"),
                    "state"
                ],
                live=True
            )

demo.launch(share=True, server_name="0.0.0.0", debug=True)